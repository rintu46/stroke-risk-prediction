# -*- coding: utf-8 -*-
"""stroke risk prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UuTCBiunGyUR3pdHZZ4aAQS672ucmBf0
"""

import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers

from google.colab import files
uploaded = files.upload()

import io

dataframe = pd.read_csv(io.BytesIO(uploaded['stroke.csv']))

dataframe.shape

dataframe.head()

dataframe = dataframe.drop('id', axis=1)

dataframe.head()

dataframe[dataframe==0].count()

dataframe['bmi'].fillna(dataframe['bmi'].mean(), inplace= True)

dataframe['smoking'].fillna(dataframe['smoking'].mode()[0], inplace=True)

dataframe.describe()

dataframe.info()

pd.pivot_table(dataframe, index= 'stroke', columns='gender', values='evrmarried', aggfunc= 'count')

pd.pivot_table(dataframe, index= 'stroke', columns='gender', values='hypertension', aggfunc= 'count')

pd.pivot_table(dataframe, index= 'stroke', columns='gender', values='hdisease', aggfunc= 'count')

pd.pivot_table(dataframe, index= 'stroke', columns='work', values='smoking', aggfunc= 'count')

dataframe['stroke'].value_counts()

from sklearn import preprocessing 

encoder = preprocessing.LabelEncoder()

for i in dataframe.columns:
    if isinstance(dataframe[i][0], str):
            dataframe[i] = encoder.fit_transform(dataframe[i])

from sklearn.preprocessing import StandardScaler 
  
scalar = StandardScaler() 
  
scalar.fit(dataframe) 
scaled_data = scalar.transform(dataframe)

dataframe.head(10)

# Commented out IPython magic to ensure Python compatibility.
 
val_dataframe = dataframe.sample(frac=0.2, random_state=1337)
train_dataframe = dataframe.drop(val_dataframe.index)

print(
    "Using %d samples for training and %d for validation"
#     % (len(train_dataframe), len(val_dataframe))
)

def dataframe_to_dataset(dataframe):
    dataframe = dataframe.copy()
    labels = dataframe.pop("stroke")
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    ds = ds.shuffle(buffer_size=len(dataframe))
    return ds


train_ds = dataframe_to_dataset(train_dataframe)
val_ds = dataframe_to_dataset(val_dataframe)

for x, y in train_ds.take(1):
    print("Input:", x)
    print("Target:", y)

train_ds = train_ds.batch(32)
val_ds = val_ds.batch(32)

from tensorflow.keras.layers.experimental.preprocessing import Normalization
from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding
from tensorflow.keras.layers.experimental.preprocessing import StringLookup


def encode_numerical_feature(feature, name, dataset):
    normalizer = Normalization()

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    normalizer.adapt(feature_ds)
    encoded_feature = normalizer(feature)
    return encoded_feature


def encode_string_categorical_feature(feature, name, dataset):
    index = StringLookup()

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    index.adapt(feature_ds)

    encoded_feature = index(feature)

    encoder = CategoryEncoding(output_mode="binary")

    feature_ds = feature_ds.map(index)
    encoder.adapt(feature_ds)

    encoded_feature = encoder(encoded_feature)
    return encoded_feature


def encode_integer_categorical_feature(feature, name, dataset):
    encoder = CategoryEncoding(output_mode="binary")

    feature_ds = dataset.map(lambda x, y: x[name])
    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))

    encoder.adapt(feature_ds)

    encoded_feature = encoder(feature)
    return encoded_feature

hypertension = keras.Input(shape=(1,), name="hypertension", dtype="int64")
hdisease = keras.Input(shape=(1,), name="hdisease", dtype="int64")
gender = keras.Input(shape=(1,), name="gender", dtype="int64")
evrmarried = keras.Input(shape=(1,), name="evrmarried", dtype="int64")
work = keras.Input(shape=(1,), name="work", dtype="int64")
Residence = keras.Input(shape=(1,), name="Residence", dtype="int64")
smoking = keras.Input(shape=(1,), name="smoking", dtype="int64")


age = keras.Input(shape=(1,), name="age")
glucose = keras.Input(shape=(1,), name="glucose")
bmi = keras.Input(shape=(1,), name="bmi")

all_inputs = [
  
    hypertension,
    hdisease,
    gender,
    evrmarried,
    work,
    Residence,
    smoking,
   #id,
    age,
    glucose,
    bmi,
]

hypertension_encoded = encode_integer_categorical_feature(hypertension, "hypertension", train_ds)
hdisease_encoded = encode_integer_categorical_feature(hdisease, "hdisease", train_ds)
gender_encoded = encode_integer_categorical_feature(gender, "gender", train_ds)
evrmarried_encoded = encode_integer_categorical_feature(evrmarried, "evrmarried", train_ds)
work_encoded = encode_integer_categorical_feature(work, "work", train_ds)
Residence_encoded = encode_integer_categorical_feature(Residence, "Residence", train_ds)
smoking_encoded = encode_integer_categorical_feature(smoking, "smoking", train_ds)

age_encoded = encode_numerical_feature(age, "age", train_ds)
glucose_encoded = encode_numerical_feature(glucose, "glucose", train_ds)
bmi_encoded = encode_numerical_feature(bmi, "bmi", train_ds)

all_features = layers.concatenate(
    [
        hypertension_encoded,
        hdisease_encoded, 
        gender_encoded,
        evrmarried_encoded,     
        work_encoded,
        Residence_encoded,  
        smoking_encoded,
        age_encoded,
        glucose_encoded,
        bmi_encoded,
     
    ]
)
x = layers.Dense(32, activation="relu")(all_features)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(all_inputs, output)
model.compile("adam", "binary_crossentropy", metrics=["accuracy"])

keras.utils.plot_model(model, show_shapes=True, rankdir="LR")

import matplotlib.pyplot as plt

history = model.fit(train_ds,epochs=10, validation_data=val_ds)
plt.plot(history.history["loss"], label="Training Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.legend()
plt.show()

plt.plot(history.history["accuracy"], label="Training Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.legend()
plt.show()

sample = {
    
    "gender": 1,
    "age": 17,
    "hypertension": 0,
    "hdisease": 1,
    "evrmarried": 1,
    "work": 1,
    "Residence": 1,
    "glucose": 228.69,
    "bmi": 36.6,
    "smoking": 1,

}

input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}
predictions = model.predict(input_dict)

print(100*predictions[0][0])

print(
    "This particular patient had a %.1f percent probability "
    "of having a stroke risk." % (100 * predictions[0][0])
)

stroke = dataframe
stroke.loc[stroke['stroke'] == 0,
                 'age'].hist(label='No Stroke')
stroke.loc[stroke['stroke'] == 1,
                 'age'].hist(label='Heart Stroke')
plt.xlabel('Age')
plt.ylabel('Heart Stroke')
plt.legend()

stroke.loc[stroke['stroke'] == 0,
                 'bmi'].hist(label='No Stroke')
stroke.loc[stroke['stroke'] == 1,
                 'bmi'].hist(label='Heart Stroke')
plt.xlabel('BMI')
plt.ylabel('Heart Stroke')
plt.legend()

import seaborn as sns
plt.figure(figsize=(28,20))
sns.relplot(x= 'glucose', y='age', hue= 'stroke', sizes= (15,200), data=stroke)
plt.xticks(rotation=90)

X= stroke.drop('stroke', axis=1)
X.shape

y= stroke['stroke']
y.shape

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3, random_state = 1000)

log= LogisticRegression()

log.fit(X_train,y_train)

log.score(X_train, y_train)

stroke['stroke'].value_counts()

stroke_copy= stroke.copy()

stroke_copy.head()

li = list(stroke_copy[stroke_copy.stroke == 0].sample(n=4180).index)

X_drop= stroke_copy.drop('stroke', axis=1)
X_drop.shape

y_drop= stroke_copy.stroke
y_drop.shape

X_droptr,X_dropts,y_droptr,y_dropts = train_test_split(X_drop,y_drop,test_size=.3, random_state = 1000)

log.score(X_droptr, y_droptr)

y_underlog= log.predict(X_dropts)

from sklearn.metrics import accuracy_score, f1_score, classification_report, recall_score, confusion_matrix
cm_log= confusion_matrix(y_dropts, y_underlog)

plt.figure(figsize=(24,12))

plt.suptitle("Confusion Matrixes After Undersampling",fontsize=24)
plt.subplots_adjust(wspace = 0.4, hspace= 0.4)

plt.subplot(2,3,1)
plt.title("Logistic Regression Confusion Matrix")
sns.heatmap(cm_log,annot=True,cmap="Blues",fmt="d",cbar=False, annot_kws={"size": 24})